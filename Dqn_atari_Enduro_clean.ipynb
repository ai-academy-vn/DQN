{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:teal\"> Deep Reinforcement Learning for Atari Enduro-v0 </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install keras-rl\n",
    "# !pip install --no-index -f https://github.com/Kojoley/atari-py/releases atari_py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from time import sleep\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout, concatenate, Permute\n",
    "from keras.layers import Input, Conv2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.activations import relu, linear\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.core import Processor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### RoadRunner Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x13b7a80bc88>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make('Enduro-v0')\n",
    "\n",
    "plt.imshow(env.render(mode='rgb_array'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. *Number of possible action*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of Possible actoin is : 9\n"
     ]
    }
   ],
   "source": [
    "nb_actions = env.action_space.n\n",
    "print('Total number of Possible actoin is :', nb_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. *Taking stack of 4 consecutive frames*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape is : (4, 84, 84)\n"
     ]
    }
   ],
   "source": [
    "frame_shape = (84, 84)\n",
    "window_length = 4\n",
    "input_shape = (window_length,) + frame_shape\n",
    "print('Input Shape is :', input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "***\n",
    "### Defining class for pre-processing the game_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GameProcess(Processor):\n",
    "    def process_observation(self, observation):\n",
    "        img = Image.fromarray(observation)\n",
    "        img = np.array(img.resize(frame_shape).convert('L'))\n",
    "        return img.astype('uint8')  \n",
    "\n",
    "    def process_state_batch(self, batch):\n",
    "        Processed_batch = batch.astype('float32') / 255.\n",
    "        return Processed_batch\n",
    "\n",
    "    def process_reward(self, reward):\n",
    "        return np.clip(reward, -1., 1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## DeepMind Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "permute_2 (Permute)          (None, 84, 84, 4)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 20, 20, 32)        8224      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 20, 20, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 9, 9, 64)          32832     \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 9, 9, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 7, 7, 64)          36928     \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               1606144   \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 9)                 4617      \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 9)                 0         \n",
      "=================================================================\n",
      "Total params: 1,688,745\n",
      "Trainable params: 1,688,745\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Permute((2, 3, 1), input_shape=input_shape))\n",
    "model.add(Conv2D(32, (8, 8), strides=(4, 4)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (4, 4), strides=(2, 2)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3), strides=(1, 1)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Configuring the Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. *Allocating memory for experience replay*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = SequentialMemory(limit=1000000, window_length=window_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.* Policy: Epsilon Greedy Exploration*\n",
    "<span style=\"color:teal\">*Gradually exploration will be decreased*</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1., value_min=.1, value_test=.05, nb_steps=1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. *Compiling DQN Agent*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, policy=policy, memory=memory, processor=GameProcess(),\n",
    "               nb_steps_warmup=50000, gamma=.99, target_model_update=10000, train_interval=4, delta_clip=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.compile(Adam(lr=.00025), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## <span style=\"color:teal\"> Training the model </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">**-  -  Caution   -  -**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 2000000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 22s 2ms/step - reward: 0.0000e+00\n",
      "2 episodes - episode_reward: 0.000 [0.000, 0.000] - ale.lives: 0.000\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 19s 2ms/step - reward: 0.0000e+00\n",
      "2 episodes - episode_reward: 0.000 [0.000, 0.000] - ale.lives: 0.000\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 19s 2ms/step - reward: 0.0000e+00\n",
      "2 episodes - episode_reward: 0.000 [0.000, 0.000] - ale.lives: 0.000\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 19s 2ms/step - reward: 0.0000e+00: 0s - reward: 0.0000e+0\n",
      "3 episodes - episode_reward: 0.000 [0.000, 0.000] - ale.lives: 0.000\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 19s 2ms/step - reward: 0.0000e+00\n",
      "2 episodes - episode_reward: 0.000 [0.000, 0.000] - ale.lives: 0.000\n",
      "\n",
      "Interval 6 (50000 steps performed)\n",
      "    1/10000 [..............................] - ETA: 1:09 - reward: 0.0000e+00WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "10000/10000 [==============================] - 63s 6ms/step - reward: 0.0000e+00\n",
      "2 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.001 - mean_absolute_error: 0.018 - mean_q: 0.007 - mean_eps: 0.951 - ale.lives: 0.000\n",
      "\n",
      "Interval 7 (60000 steps performed)\n",
      "10000/10000 [==============================] - 62s 6ms/step - reward: 0.0000e+00\n",
      "2 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.010 - mean_q: -0.011 - mean_eps: 0.942 - ale.lives: 0.000\n",
      "\n",
      "Interval 8 (70000 steps performed)\n",
      "10000/10000 [==============================] - 62s 6ms/step - reward: 0.0000e+00\n",
      "3 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.010 - mean_q: -0.011 - mean_eps: 0.933 - ale.lives: 0.000\n",
      "\n",
      "Interval 9 (80000 steps performed)\n",
      "10000/10000 [==============================] - 63s 6ms/step - reward: 0.0000e+00\n",
      "2 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.010 - mean_q: -0.011 - mean_eps: 0.924 - ale.lives: 0.000\n",
      "\n",
      "Interval 10 (90000 steps performed)\n",
      "10000/10000 [==============================] - 62s 6ms/step - reward: 0.0000e+00\n",
      "2 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.009 - mean_q: -0.010 - mean_eps: 0.915 - ale.lives: 0.000\n",
      "\n",
      "Interval 11 (100000 steps performed)\n",
      "10000/10000 [==============================] - 62s 6ms/step - reward: 0.0000e+00\n",
      "2 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.009 - mean_q: -0.010 - mean_eps: 0.906 - ale.lives: 0.000\n",
      "\n",
      "Interval 12 (110000 steps performed)\n",
      "10000/10000 [==============================] - 62s 6ms/step - reward: 0.0000e+00\n",
      "3 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.009 - mean_q: -0.010 - mean_eps: 0.897 - ale.lives: 0.000\n",
      "\n",
      "Interval 13 (120000 steps performed)\n",
      "10000/10000 [==============================] - 63s 6ms/step - reward: 0.0000e+00\n",
      "2 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.009 - mean_q: -0.010 - mean_eps: 0.888 - ale.lives: 0.000\n",
      "\n",
      "Interval 14 (130000 steps performed)\n",
      "10000/10000 [==============================] - 62s 6ms/step - reward: 0.0000e+00\n",
      "2 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.009 - mean_q: -0.010 - mean_eps: 0.879 - ale.lives: 0.000\n",
      "\n",
      "Interval 15 (140000 steps performed)\n",
      "10000/10000 [==============================] - 62s 6ms/step - reward: 0.0000e+00\n",
      "2 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.009 - mean_q: -0.010 - mean_eps: 0.870 - ale.lives: 0.000\n",
      "\n",
      "Interval 16 (150000 steps performed)\n",
      "10000/10000 [==============================] - 62s 6ms/step - reward: 0.0000e+00\n",
      "3 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.009 - mean_q: -0.010 - mean_eps: 0.861 - ale.lives: 0.000\n",
      "\n",
      "Interval 17 (160000 steps performed)\n",
      "10000/10000 [==============================] - 62s 6ms/step - reward: 0.0000e+00\n",
      "2 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.009 - mean_q: -0.010 - mean_eps: 0.852 - ale.lives: 0.000\n",
      "\n",
      "Interval 18 (170000 steps performed)\n",
      "10000/10000 [==============================] - 63s 6ms/step - reward: 0.0000e+00\n",
      "2 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.009 - mean_q: -0.010 - mean_eps: 0.843 - ale.lives: 0.000\n",
      "\n",
      "Interval 19 (180000 steps performed)\n",
      "10000/10000 [==============================] - 63s 6ms/step - reward: 0.0000e+00\n",
      "2 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.009 - mean_q: -0.010 - mean_eps: 0.834 - ale.lives: 0.000\n",
      "\n",
      "Interval 20 (190000 steps performed)\n",
      "10000/10000 [==============================] - 61s 6ms/step - reward: 0.0000e+00\n",
      "3 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.009 - mean_q: -0.010 - mean_eps: 0.825 - ale.lives: 0.000\n",
      "\n",
      "Interval 21 (200000 steps performed)\n",
      "10000/10000 [==============================] - 62s 6ms/step - reward: 0.0000e+00\n",
      "2 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.009 - mean_q: -0.010 - mean_eps: 0.816 - ale.lives: 0.000\n",
      "\n",
      "Interval 22 (210000 steps performed)\n",
      "10000/10000 [==============================] - 61s 6ms/step - reward: 0.0000e+00\n",
      "2 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.009 - mean_q: -0.009 - mean_eps: 0.807 - ale.lives: 0.000\n",
      "\n",
      "Interval 23 (220000 steps performed)\n",
      "10000/10000 [==============================] - 62s 6ms/step - reward: 0.0000e+00\n",
      "2 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.008 - mean_q: -0.009 - mean_eps: 0.798 - ale.lives: 0.000\n",
      "\n",
      "Interval 24 (230000 steps performed)\n",
      "10000/10000 [==============================] - 62s 6ms/step - reward: 0.0000e+00\n",
      "3 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.008 - mean_q: -0.009 - mean_eps: 0.789 - ale.lives: 0.000\n",
      "\n",
      "Interval 25 (240000 steps performed)\n",
      "10000/10000 [==============================] - 61s 6ms/step - reward: 0.0000e+00\n",
      "2 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.008 - mean_q: -0.009 - mean_eps: 0.780 - ale.lives: 0.000\n",
      "\n",
      "Interval 26 (250000 steps performed)\n",
      "10000/10000 [==============================] - 62s 6ms/step - reward: 0.0000e+00\n",
      "2 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.008 - mean_q: -0.009 - mean_eps: 0.771 - ale.lives: 0.000\n",
      "\n",
      "Interval 27 (260000 steps performed)\n",
      "10000/10000 [==============================] - 62s 6ms/step - reward: 0.0000e+00\n",
      "2 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.008 - mean_q: -0.009 - mean_eps: 0.762 - ale.lives: 0.000\n",
      "\n",
      "Interval 28 (270000 steps performed)\n",
      "10000/10000 [==============================] - 62s 6ms/step - reward: 0.0000e+00\n",
      "3 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.008 - mean_q: -0.009 - mean_eps: 0.753 - ale.lives: 0.000\n",
      "\n",
      "Interval 29 (280000 steps performed)\n",
      "10000/10000 [==============================] - 62s 6ms/step - reward: 0.0000e+00\n",
      "2 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.008 - mean_q: -0.009 - mean_eps: 0.744 - ale.lives: 0.000\n",
      "\n",
      "Interval 30 (290000 steps performed)\n",
      "10000/10000 [==============================] - 63s 6ms/step - reward: 0.0000e+00\n",
      "2 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.008 - mean_q: -0.009 - mean_eps: 0.735 - ale.lives: 0.000\n",
      "\n",
      "Interval 31 (300000 steps performed)\n",
      "10000/10000 [==============================] - 63s 6ms/step - reward: 0.0000e+00\n",
      "2 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.007 - mean_q: -0.008 - mean_eps: 0.726 - ale.lives: 0.000\n",
      "\n",
      "Interval 32 (310000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 63s 6ms/step - reward: 0.0000e+00\n",
      "3 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.008 - mean_q: -0.008 - mean_eps: 0.717 - ale.lives: 0.000\n",
      "\n",
      "Interval 33 (320000 steps performed)\n",
      "10000/10000 [==============================] - 62s 6ms/step - reward: 0.0000e+00\n",
      "2 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.007 - mean_q: -0.008 - mean_eps: 0.708 - ale.lives: 0.000\n",
      "\n",
      "Interval 34 (330000 steps performed)\n",
      "10000/10000 [==============================] - 62s 6ms/step - reward: 0.0000e+00\n",
      "2 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.007 - mean_q: -0.008 - mean_eps: 0.699 - ale.lives: 0.000\n",
      "\n",
      "Interval 35 (340000 steps performed)\n",
      "10000/10000 [==============================] - 63s 6ms/step - reward: 0.0000e+00\n",
      "2 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.007 - mean_q: -0.008 - mean_eps: 0.690 - ale.lives: 0.000\n",
      "\n",
      "Interval 36 (350000 steps performed)\n",
      "10000/10000 [==============================] - 61s 6ms/step - reward: 0.0000e+00\n",
      "3 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.008 - mean_q: -0.008 - mean_eps: 0.681 - ale.lives: 0.000\n",
      "\n",
      "Interval 37 (360000 steps performed)\n",
      "10000/10000 [==============================] - 63s 6ms/step - reward: 0.0000e+00\n",
      "2 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.007 - mean_q: -0.008 - mean_eps: 0.672 - ale.lives: 0.000\n",
      "\n",
      "Interval 38 (370000 steps performed)\n",
      "10000/10000 [==============================] - 63s 6ms/step - reward: 0.0000e+00\n",
      "2 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.007 - mean_q: -0.008 - mean_eps: 0.663 - ale.lives: 0.000\n",
      "\n",
      "Interval 39 (380000 steps performed)\n",
      "10000/10000 [==============================] - 63s 6ms/step - reward: 0.0000e+00\n",
      "2 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.007 - mean_q: -0.008 - mean_eps: 0.654 - ale.lives: 0.000\n",
      "\n",
      "Interval 40 (390000 steps performed)\n",
      "10000/10000 [==============================] - 64s 6ms/step - reward: 0.0000e+00\n",
      "3 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.007 - mean_q: -0.008 - mean_eps: 0.645 - ale.lives: 0.000\n",
      "\n",
      "Interval 41 (400000 steps performed)\n",
      "10000/10000 [==============================] - 63s 6ms/step - reward: 0.0000e+00\n",
      "2 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.007 - mean_q: -0.008 - mean_eps: 0.636 - ale.lives: 0.000\n",
      "\n",
      "Interval 42 (410000 steps performed)\n",
      "10000/10000 [==============================] - 62s 6ms/step - reward: 0.0000e+00\n",
      "2 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.007 - mean_q: -0.008 - mean_eps: 0.627 - ale.lives: 0.000\n",
      "\n",
      "Interval 43 (420000 steps performed)\n",
      "10000/10000 [==============================] - 63s 6ms/step - reward: 0.0000e+00\n",
      "2 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.007 - mean_q: -0.008 - mean_eps: 0.618 - ale.lives: 0.000\n",
      "\n",
      "Interval 44 (430000 steps performed)\n",
      "10000/10000 [==============================] - 62s 6ms/step - reward: 0.0000e+00\n",
      "3 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.007 - mean_q: -0.008 - mean_eps: 0.609 - ale.lives: 0.000\n",
      "\n",
      "Interval 45 (440000 steps performed)\n",
      "10000/10000 [==============================] - 64s 6ms/step - reward: 0.0000e+00\n",
      "2 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.007 - mean_q: -0.008 - mean_eps: 0.600 - ale.lives: 0.000\n",
      "\n",
      "Interval 46 (450000 steps performed)\n",
      "10000/10000 [==============================] - 63s 6ms/step - reward: 0.0000e+00\n",
      "2 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.007 - mean_q: -0.008 - mean_eps: 0.591 - ale.lives: 0.000\n",
      "\n",
      "Interval 47 (460000 steps performed)\n",
      "10000/10000 [==============================] - 62s 6ms/step - reward: 0.0000e+00\n",
      "2 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.007 - mean_q: -0.008 - mean_eps: 0.582 - ale.lives: 0.000\n",
      "\n",
      "Interval 48 (470000 steps performed)\n",
      "10000/10000 [==============================] - 63s 6ms/step - reward: 0.0000e+00\n",
      "3 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.007 - mean_q: -0.008 - mean_eps: 0.573 - ale.lives: 0.000\n",
      "\n",
      "Interval 49 (480000 steps performed)\n",
      "10000/10000 [==============================] - 63s 6ms/step - reward: 0.0000e+00\n",
      "2 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.007 - mean_q: -0.008 - mean_eps: 0.564 - ale.lives: 0.000\n",
      "\n",
      "Interval 50 (490000 steps performed)\n",
      "10000/10000 [==============================] - 63s 6ms/step - reward: 0.0000e+00\n",
      "2 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.007 - mean_q: -0.008 - mean_eps: 0.555 - ale.lives: 0.000\n",
      "\n",
      "Interval 51 (500000 steps performed)\n",
      "10000/10000 [==============================] - 64s 6ms/step - reward: 0.0000e+00\n",
      "2 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.007 - mean_q: -0.008 - mean_eps: 0.546 - ale.lives: 0.000\n",
      "\n",
      "Interval 52 (510000 steps performed)\n",
      "10000/10000 [==============================] - 64s 6ms/step - reward: 0.0000e+00\n",
      "3 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.007 - mean_q: -0.008 - mean_eps: 0.537 - ale.lives: 0.000\n",
      "\n",
      "Interval 53 (520000 steps performed)\n",
      "10000/10000 [==============================] - 63s 6ms/step - reward: 0.0000e+00\n",
      "2 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.007 - mean_q: -0.008 - mean_eps: 0.528 - ale.lives: 0.000\n",
      "\n",
      "Interval 54 (530000 steps performed)\n",
      "10000/10000 [==============================] - 64s 6ms/step - reward: 0.0000e+00\n",
      "2 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.007 - mean_q: -0.008 - mean_eps: 0.519 - ale.lives: 0.000\n",
      "\n",
      "Interval 55 (540000 steps performed)\n",
      "10000/10000 [==============================] - 64s 6ms/step - reward: 0.0000e+00\n",
      "2 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.007 - mean_q: -0.008 - mean_eps: 0.510 - ale.lives: 0.000\n",
      "\n",
      "Interval 56 (550000 steps performed)\n",
      "10000/10000 [==============================] - 63s 6ms/step - reward: 0.0000e+00: 0s - reward: 0\n",
      "3 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.007 - mean_q: -0.007 - mean_eps: 0.501 - ale.lives: 0.000\n",
      "\n",
      "Interval 57 (560000 steps performed)\n",
      "10000/10000 [==============================] - 63s 6ms/step - reward: 0.0000e+00\n",
      "2 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.007 - mean_q: -0.007 - mean_eps: 0.492 - ale.lives: 0.000\n",
      "\n",
      "Interval 58 (570000 steps performed)\n",
      "10000/10000 [==============================] - 62s 6ms/step - reward: 0.0000e+00\n",
      "2 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.006 - mean_q: -0.007 - mean_eps: 0.483 - ale.lives: 0.000\n",
      "\n",
      "Interval 59 (580000 steps performed)\n",
      "10000/10000 [==============================] - 63s 6ms/step - reward: 0.0000e+00\n",
      "2 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.006 - mean_q: -0.007 - mean_eps: 0.474 - ale.lives: 0.000\n",
      "\n",
      "Interval 60 (590000 steps performed)\n",
      "10000/10000 [==============================] - 64s 6ms/step - reward: 0.0000e+00\n",
      "3 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.006 - mean_q: -0.007 - mean_eps: 0.465 - ale.lives: 0.000\n",
      "\n",
      "Interval 61 (600000 steps performed)\n",
      "10000/10000 [==============================] - 63s 6ms/step - reward: 0.0000e+00\n",
      "2 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.006 - mean_q: -0.007 - mean_eps: 0.456 - ale.lives: 0.000\n",
      "\n",
      "Interval 62 (610000 steps performed)\n",
      "10000/10000 [==============================] - 63s 6ms/step - reward: 0.0000e+00\n",
      "2 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.006 - mean_q: -0.007 - mean_eps: 0.447 - ale.lives: 0.000\n",
      "\n",
      "Interval 63 (620000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 64s 6ms/step - reward: 0.0000e+00\n",
      "2 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.006 - mean_q: -0.006 - mean_eps: 0.438 - ale.lives: 0.000\n",
      "\n",
      "Interval 64 (630000 steps performed)\n",
      "10000/10000 [==============================] - 65s 6ms/step - reward: 0.0000e+00\n",
      "3 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.006 - mean_q: -0.006 - mean_eps: 0.429 - ale.lives: 0.000\n",
      "\n",
      "Interval 65 (640000 steps performed)\n",
      "10000/10000 [==============================] - 65s 6ms/step - reward: 0.0000e+00\n",
      "2 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.006 - mean_q: -0.006 - mean_eps: 0.420 - ale.lives: 0.000\n",
      "\n",
      "Interval 66 (650000 steps performed)\n",
      "10000/10000 [==============================] - 63s 6ms/step - reward: 0.0000e+00\n",
      "2 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.006 - mean_q: -0.006 - mean_eps: 0.411 - ale.lives: 0.000\n",
      "\n",
      "Interval 67 (660000 steps performed)\n",
      "10000/10000 [==============================] - 63s 6ms/step - reward: 0.0000e+00\n",
      "3 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.006 - mean_q: -0.006 - mean_eps: 0.402 - ale.lives: 0.000\n",
      "\n",
      "Interval 68 (670000 steps performed)\n",
      "10000/10000 [==============================] - 63s 6ms/step - reward: 0.0000e+00\n",
      "2 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.006 - mean_q: -0.006 - mean_eps: 0.393 - ale.lives: 0.000\n",
      "\n",
      "Interval 69 (680000 steps performed)\n",
      "10000/10000 [==============================] - 64s 6ms/step - reward: 0.0000e+00\n",
      "2 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.006 - mean_q: -0.006 - mean_eps: 0.384 - ale.lives: 0.000\n",
      "\n",
      "Interval 70 (690000 steps performed)\n",
      "10000/10000 [==============================] - 64s 6ms/step - reward: 0.0000e+00\n",
      "2 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.006 - mean_q: -0.006 - mean_eps: 0.375 - ale.lives: 0.000\n",
      "\n",
      "Interval 71 (700000 steps performed)\n",
      "10000/10000 [==============================] - 64s 6ms/step - reward: 0.0000e+00\n",
      "3 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.006 - mean_q: -0.006 - mean_eps: 0.366 - ale.lives: 0.000\n",
      "\n",
      "Interval 72 (710000 steps performed)\n",
      "10000/10000 [==============================] - 64s 6ms/step - reward: 0.0000e+00\n",
      "2 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.006 - mean_q: -0.006 - mean_eps: 0.357 - ale.lives: 0.000\n",
      "\n",
      "Interval 73 (720000 steps performed)\n",
      "10000/10000 [==============================] - 65s 7ms/step - reward: 0.0000e+00\n",
      "2 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.005 - mean_q: -0.006 - mean_eps: 0.348 - ale.lives: 0.000\n",
      "\n",
      "Interval 74 (730000 steps performed)\n",
      "10000/10000 [==============================] - 64s 6ms/step - reward: 0.0000e+00\n",
      "2 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.006 - mean_q: -0.006 - mean_eps: 0.339 - ale.lives: 0.000\n",
      "\n",
      "Interval 75 (740000 steps performed)\n",
      "10000/10000 [==============================] - 65s 6ms/step - reward: 0.0000e+00\n",
      "3 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.006 - mean_q: -0.006 - mean_eps: 0.330 - ale.lives: 0.000\n",
      "\n",
      "Interval 76 (750000 steps performed)\n",
      "10000/10000 [==============================] - 65s 6ms/step - reward: 2.0000e-04\n",
      "2 episodes - episode_reward: 0.500 [0.000, 1.000] - loss: 0.000 - mean_absolute_error: 0.006 - mean_q: -0.006 - mean_eps: 0.321 - ale.lives: 0.000\n",
      "\n",
      "Interval 77 (760000 steps performed)\n",
      "10000/10000 [==============================] - 64s 6ms/step - reward: -1.0000e-04\n",
      "2 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.005 - mean_q: -0.006 - mean_eps: 0.312 - ale.lives: 0.000\n",
      "\n",
      "Interval 78 (770000 steps performed)\n",
      "10000/10000 [==============================] - 65s 6ms/step - reward: 0.0000e+00\n",
      "2 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.005 - mean_q: -0.006 - mean_eps: 0.303 - ale.lives: 0.000\n",
      "\n",
      "Interval 79 (780000 steps performed)\n",
      "10000/10000 [==============================] - 64s 6ms/step - reward: 0.0000e+00\n",
      "3 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.005 - mean_q: -0.005 - mean_eps: 0.294 - ale.lives: 0.000\n",
      "\n",
      "Interval 80 (790000 steps performed)\n",
      "10000/10000 [==============================] - 64s 6ms/step - reward: 0.0000e+00\n",
      "2 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.005 - mean_q: -0.006 - mean_eps: 0.285 - ale.lives: 0.000\n",
      "\n",
      "Interval 81 (800000 steps performed)\n",
      "10000/10000 [==============================] - 63s 6ms/step - reward: 1.0000e-04\n",
      "2 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.006 - mean_q: -0.006 - mean_eps: 0.276 - ale.lives: 0.000\n",
      "\n",
      "Interval 82 (810000 steps performed)\n",
      "10000/10000 [==============================] - 64s 6ms/step - reward: 2.0000e-04\n",
      "2 episodes - episode_reward: 1.500 [0.000, 3.000] - loss: 0.000 - mean_absolute_error: 0.008 - mean_q: -0.008 - mean_eps: 0.267 - ale.lives: 0.000\n",
      "\n",
      "Interval 83 (820000 steps performed)\n",
      "10000/10000 [==============================] - 64s 6ms/step - reward: 0.0035\n",
      "3 episodes - episode_reward: 11.667 [0.000, 32.000] - loss: 0.000 - mean_absolute_error: 0.008 - mean_q: -0.008 - mean_eps: 0.258 - ale.lives: 0.000\n",
      "\n",
      "Interval 84 (830000 steps performed)\n",
      "10000/10000 [==============================] - 64s 6ms/step - reward: 3.0000e-04\n",
      "2 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mean_absolute_error: 0.008 - mean_q: -0.007 - mean_eps: 0.249 - ale.lives: 0.000\n",
      "\n",
      "Interval 85 (840000 steps performed)\n",
      " 5725/10000 [================>.............] - ETA: 27s - reward: 6.9869e-04done, took 5113.968 seconds\n"
     ]
    }
   ],
   "source": [
    "history = dqn.fit(env, nb_steps=2000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\">**-  -  -  Safe  -  -  -**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.load_weights('dqn_atari_Enduro_.h5f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## <span style=\"color:teal\"> Testing the model </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 1 episodes ...\n",
      "Episode 1: reward: 174.000, steps: 4423\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAANEAAAD8CAYAAADpCEEHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAEl1JREFUeJzt3W2sHOV5xvH/VROQSlLxakSwAYMcpJOochyURE2xHFIIOYrq0A+VrTaQFslBjSVTp0qc8qEon2gaTuKohRYaKxBSoG1CQe1JAiI4ECm8GGKMMXFsCGkOWLZCXkhflNTm7oeZhTnr3bNz9tndmdm9ftJod5+d3Xlmd69zz8yZfVYRgZn17zeq7oBZ0zlEZokcIrNEDpFZIofILJFDZJZoaCGSdJmkfZIOSNo6rOWYVU3D+D+RpCXAD4BLgDngcWBDROwd+MLMKjasSvRO4EBEPB8RvwbuBNYNaVlmlTpuSM97FvDjwu054F3dZpa0YDlcunTpgLplVt7hw4d/EhGn95pvWCFSh7Z5QZG0EdhY5sk2bNgwiD6ZLcq2bdt+VGa+YYVoDlheuL0MeKk4Q0TcDNwMvSuRWZ0Na5/ocWClpBWSjgfWA/cOaVlmlRpKJYqII5I2Ad8ElgDbI+KZYSzLrGrD2pwjImaB2WE9v1ld+IwFs0QOkVkih8gskUNklsghMkvkEJklcojMEjlEZokcIrNEDpFZIofILJFDZJbIITJL5BCZJXKIzBI5RGaJHCKzRH2HSNJySQ9KelbSM5I25+3XSXpR0q58mh5cd83qJ+Xr4UeAj0fEk5LeBDwh6f78vs9FxGfTu2dWf32HKCIOAgfz67+U9CzZoI1mE2Ug+0SSzgXeDjyaN22StFvSdkknD2IZZnWVHCJJbwS+ClwTEa8ANwHnA6vIKtUNXR63UdJOSTtT+2BWpaQQSXoDWYC+EhFfA4iIQxFxNCJeBW4hG9z+GBFxc0RcGBEXpvTBrGopR+cEfBF4NiJmCu1nFma7HNjTf/fM6i/l6Nx7gA8DT0valbf9JbBB0iqyAexfAD6a1EOzmks5OvcdOv/6g0c9tYniMxbMEjlEZokcIrNEDpFZIofILJFDZJbIITJL5BCZJXKIzBI5RGaJHCKzRA6RWSKHyCyRQ2SWyCEyS5TypTyzvlw6981j2u5b9v4KejIYDlGb9je4yW9u3XQKT/t9TXy9k0Mk6QXgl8BR4EhEXCjpFOAu4Fyyr4j/YUT8LHVZw9TtDV7ojW/iG16FhV7DbvO2v7Z1/uM2qEr03oj4SeH2VuCBiLhe0tb89icHtKwki3lDyz5Xnd7QOkl5rXs9tk6v/bA259YBa/PrtwI7qDhEgwxPt+euwxtaB8N8rbstq8rXfhAhCuA+SQH8Q0TcDJyRDzNMRByUtHQAy+nLpL2hVRrla91t2ZW89hGRNAFvzi+XAk8Ba4Cft83zsw6P2wjszKfw5KmG084yGUj+P1FEvJRfHgbuJhvx9FBrEMf88nCHx3kE1IpNT/tXbwYhdRjhE/OfVUHSicClZCOe3gtcmc92JXBPynLM6iy1Ep0BfEfSU8BjwH9ExDeA64FLJO0HLslvWw25Gg1A6j7RICaq3/aduGl6enrepaeO02j2iazZZmdnmZ6edkVK4BBNoOnpaWZnPWT6oDhExuzs7GsVyRbPIZowrkKD5xDZa1yN+uMQTRBXoeFwiGweV6PFc4gmhKvQ8DhEdgxXo8VxiCaAq9BwOURmiRwi68ibdOU5RGPOm3LD5xBZV65G5ThEY8xVaDQcIluQq1FvDtGYchUanb5DJOkCSbsK0yuSrpF0naQXC+3+M9ZwrkYL63vcuYjYB6wCkLQEeJFstJ8/AT4XEZ8dSA9t0VyFRmtQm3PvA56LiB8N6PmsZlyNuhtUiNYDdxRub5K0W9J2SScPaBlWgqvQ6CWHSNLxwO8D/5I33QScT7apdxC4ocvjNkraKWlnah9sNFyNOhtEJfoA8GREHAKIiEMRcTQiXgVuIRsR9RgeAXXwXIWqMYgQbaCwKdcaPjh3OdmIqNYgt902xW23TXW8z9XoWKnDCP8m2QinXys0f0bS05J2A+8F/jxlGVbOMKpQtyDZfEk/rRIR/wOc2tb24aQeWaWKwbniir0d52lVI286ZvybrWNgkB/obsGx7nzaj/XF+0avc4gazptV1XOIrG+uRhmHqMFcherBIbIkrkYOUWO5CtWHQ9RQK1eurLoLr5mdnWXz5s1Vd6MyDlEDbd68mW3btlXdDcs5RDYQ27Ztm9hq5BA1jKtQ/ThEZokcIhuYSd2kc4gaxJty9eQQ2UBNYjVyiBrCVai+HCIbuEmrRqVClA99dVjSnkLbKZLul7Q/vzw5b5ekL0g6kA+btXpYnZ8UrkL1VrYSfQm4rK1tK/BARKwEHshvQzb6z8p82kg2hJZNmEmqRqVCFBEPAT9ta14H3JpfvxX4UKH9tsg8ApzUNgKQLYKrUP2l7BOdEREHAfLLpXn7WcCPC/PN5W3zePDG8Tcp1WgYBxbUoS2OafDgjT3NzMyUqkIXb3mIi7c8NIIeLd4555xTdReGLiVEh1qbafnl4bx9DlhemG8Z8FLCcqyL9vDUMUxbtmxhZmam6m4MVUqI7gWuzK9fCdxTaL8iP0r3buAXrc0+K2dmZoaZmRm2bNnSdZ6LtzzEt2bWjLBX1k3ZQ9x3AN8FLpA0J+kq4HrgEkn7yUZBvT6ffRZ4HjhANhb3nw2818a3ZtZ0rDrd2qs07tWo1OCNEbGhy13v6zBvAB9L6dQk61WBWlqVqD0wrlCj5zMWGqpJlQjGvBpFROUT2dG7iZ9mZmYq74PXb960s8zn15XIRmZcq5FDVBNl94WsfhwiG6lWNRqniuQQ1YCrULM5RDZyW7ZsGav9I4eoYq5CzecQWWXGpRo5RBVyFRoPDpFVahyqkUNklsghskqNwyatQ2SWyCGq0DjsD5hDVLnVqyd7WL5xWH+HyCxVie/6bCcbhGRPoe1vgO8Du4G7gZPy9nOB/wV25dPf+/tE3acdO3ZU3oc6TDV+HQb2faIvcezop/cDb4uI3wZ+AHyqcN9zEbEqn64u8fxmjdYzRJ1GP42I+yLiSH7zEbJhsWwRduzYwdq1a6vuRi2sXbuWHTt2VN2Nvg1in+hPga8Xbq+Q9D1J35Z0UbcHeQRUGxsl91nOpbBPVGi/lmyfSPntE4BT8+vvIBtO+Le8T9SYfQC/LvOn4Y6xIOlK4IPAH0UrCRG/ioiX8+tPAM8Bb+l3GWaN0E8lIjvQsBc4vW2+04El+fXzgBeBU1yJav3XtlZTzV6fUpWo5+CN+eina4HTJM0Bf0V2NO4E4H5JAI/kR+LWAJ+WdAQ4ClwdEe0/yWI2XhY7RtwwJqr/izOJf2VrO9XodfK4c2YjUXUVmpRKVKO/ro2YavJ6uRKZjUTVVWgSKlFN/qo2bqrB6+ZKZDYKrTMNqu2EVH0nhmQU58i1zjtrLaf9PLT29iads1fxOYZPRInfFC71I1/WDN1O4mzyyZ1N4M25IfKZ2umacIa3K9EY6bU519LEzbo68z7RkNRhX6ibJoanoqpeap/IIRqCUb/hiw1PUytRBUFyiEatqvBMQiVqGfFr7BCNUpUHESalErWM8LV2iEalqgBNYiUqGsHrXipEPsRtlsiHuBNVuRnXbwVq+uZc3ThEDdYtDON02s9CWv+IrXx9Spxh3WkE1OvIxk9ojXQ6XbjvU8ABYB/w/nE+i7sGZxlP/DTk92CoI6ACfC5eH+l0FkDSFLAeeGv+mBslLSmxDLO+1OG0oL5GQF3AOuDOyIbO+iFZRXpnQv9qqxabEVYLKUfnNknaLWm7pJPztrPIBmxsmcvbjuERUG1Qqq5G/YboJuB8YBVwELghb1eHeaPTE0TEzRFxYZnj8HXjKmRFfYUoIg5FxNGIeBW4hdc32eaA5YVZlwEvpXXRrLcqq1FfIZJ0ZuHm5cCe/Pq9wHpJJ0haAawEHkvrYr24Clm7fkdAXStpFdmm2gvARwEi4hlJ/0w2xPAR4GMRcXQ4XTebr6r/G/ncObPufO6c2Sg4RGaJHCKzRA6RWSKHyCyRvwoxQHe8+c1Vd6EWNrw0Wf9fdyUyS+QQmSXy5twILfvE+VV3oZSLrnkYgIc/f1HH+7zZOp9DVEOtD3FR+we6fZ7U+3vN0ylQlvHmXM20f3BbH95ie7FS9Ht/675iOBya/rgSTaAywWkFrahT9TKHaCw9/PmLuOiah5M/9A5NOQ7RmOpWbTrN17q/vfq4EpXjfaIx1OvDXiYMxX2phY7WmUNklqzMN1u3Ax8EDkfE2/K2u4AL8llOAn4eEasknQs8SzZwI8AjEXH1oDs9zto3r4rt7fP0ur/9eRe6v30eK6/nN1slrQH+C7itFaK2+28AfhERn85D9O+d5uuxjLH4Zmuvf0I2/Z+trfZe6zlG584N7qdVuoVDkoD/BC6OiP0Okf+TD5MXotR9oouAQxGxv9C2QtL3JH1bkrcPbOylHuLeANxRuH0QODsiXpb0DuDfJL01Il5pf6CkjcDGxOWbVa7vSiTpOOAPgLtabfkY3C/n158AngPe0unxTR4B1awoZXPu94DvR8Rcq0HS6a1fgZB0Htngjc+nddGs3nqGKB+88bvABZLmJF2V37We+ZtyAGuA3ZKeAv4VuDoiyv6ihFkj9dwniogNXdo/0qHtq8BX07s1nnod4p77zHPJzzEKZfo5SXzuXEWqPpVmEN9ZsoxDVIGqT+RsP9G00wmonULe6esR5nPnKlH1B7Hf5Vfd77pyJZpQZc+d87dde3OIJtBi9sfavxLhIB3Lm3NmiVyJKtBp0JCq+9HS6kuvr1vY6xyiClT9YSy7/Kr72RQO0QgN4p+U/kdn/XifyCyRQ2SWyD98bNadf/jYbBQcIrNEDpFZIofILNHY/p/oG6tXD/T5zr79dgCmpqbYu3fvgvNOTU3Nu73Q/O3zWvOU+Xr4ckkPSnpW0jOSNuftp0i6X9L+/PLkvF2SviDpgKTdkgb7aTarmTKV6Ajw8Yh4UtKbgCck3Q98BHggIq6XtBXYCnwS+ADZACUrgXcBN+WXI3XkslcH9lzn/fE/zbtdrB579+495nZR67YrzvjqWYki4mBEPJlf/yXZWNtnAeuAW/PZbgU+lF9fRzbkcETEI8BJks4ceM9HpD1AkAWjW1g6tbUC1Hpcq7142d5efL5em49WrUXtE+XDBL8deBQ4IyIOQhY0SUvz2c4Cflx42FzedjC1s4tx9vqTkp/juONuBDrvB/WqLMUAFatVt8d1WkZ7lbN6Kh0iSW8kG8nnmoh4JRuGu/OsHdqOOSNh2COg/uNjaQcer/6dv33t+mIrwUKbcIvdvHMVqr9SnzRJbyAL0Fci4mt586HWZlp+eThvnwOWFx6+DDhmhPM6j4DaCtDU1NRr02J1esxin8tVqBnK/D6RgC8Cz0bETOGue4Ergevzy3sK7Zsk3Ul2QOEXrc2+UXrwxk/09bi7bj8bKLe5ttCmWbfHLPR83Z7L1ajmImLBCfhdss2x3cCufJoGTgUeAPbnl6fk8wv4O7JxuJ8GLiyxjPDkqYbTzl6f3YjwWdxmC/BZ3Gaj4BCZJXKIzBI5RGaJHCKzRHX5KsRPgP/OL8fFaYzP+ozTukD59TmnzJPV4hA3gKSddTx7oV/jtD7jtC4w+PXx5pxZIofILFGdQnRz1R0YsHFan3FaFxjw+tRmn8isqepUicwaqfIQSbpM0r58YJOtVfenH5JekPS0pF2SduZtHQdyqSNJ2yUdlrSn0NbYgWi6rM91kl7M36NdkqYL930qX599kt6/6AWWOdV7WBOwhOwrE+cBxwNPAVNV9qnP9XgBOK2t7TPA1vz6VuCvq+7nAv1fA6wG9vTqP9nXYL5O9pWXdwOPVt3/kutzHfAXHeadyj93JwAr8s/jksUsr+pK9E7gQEQ8HxG/Bu4kG+hkHHQbyKV2IuIh4KdtzY0diKbL+nSzDrgzIn4VET8EDpB9LkurOkTdBjVpmgDuk/REPnYEtA3kAizt+uh66tb/Jr9nm/JN0O2Fzevk9ak6RKUGNWmA90TEarIx9z4maU3VHRqipr5nNwHnA6vIRp66IW9PXp+qQ1RqUJO6i4iX8svDwN1kmwPdBnJpiqSBaOomIg5FxNGIeBW4hdc32ZLXp+oQPQ6slLRC0vHAerKBThpD0on5yLBIOhG4FNjD6wO5wPyBXJqiW//vBa7Ij9K9m4oGolmstv22y8neI8jWZ72kEyStIBu597FFPXkNjqRMAz8gOypybdX96aP/55Ed3XkKeKa1DnQZyKWOE3AH2SbO/5H9Zb6qW//pYyCamqzPl/P+7s6Dc2Zh/mvz9dkHfGCxy/MZC2aJqt6cM2s8h8gskUNklsghMkvkEJklcojMEjlEZokcIrNE/w8yEflKBQEL9gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env.reset()\n",
    "\n",
    "dqn.test(env, nb_episodes=1, visualize=True)\n",
    "\n",
    "plt.imshow(env.render(mode='rgb_array'))\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Trained on: Intel® Xeon® Processor E5, 2.40 GHz, Nvidia Quadro K4200**\n",
    "***\n",
    "<span style=\"color:teal\"> [Bhartendu Thakur](https://github.com/matrixBT), Machine Learning & Computing</span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
